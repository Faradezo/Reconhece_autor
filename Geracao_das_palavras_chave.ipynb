{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b03bde95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: unidecode in c:\\users\\faradezo\\anaconda3\\lib\\site-packages (1.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Faradezo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\Faradezo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "\n",
    "import glob, os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from unidecode import unidecode\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8e356ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files(folder):\n",
    "    file_list = []\n",
    "    if os.path.exists(folder):\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            for file in files:\n",
    "                file_list.append(os.path.join(root,file))\n",
    "    return file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f02fc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "colunistas = get_all_files('textos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f0a3305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/alexandre_garcia.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/arnaldo_jabor.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Carla_Araujo.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Josias_de_Souza.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Juca_Kfouri.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Kennedy_Alencar.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Leonardo_Sakamoto.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Mauricio_Stycer.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Reinaldo_Azevedo.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Ricardo_Kotscho.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Tales_Faria.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Thays_Oyama.json\n",
      "https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/Wálter_Maierovitch.json\n"
     ]
    }
   ],
   "source": [
    "df_col = []\n",
    "for enu, colunista in enumerate(colunistas):\n",
    "    print(f'https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/{colunista[7:]}')\n",
    "    #df_col[enu] = pd.read_json('https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/{colunista[7:]}')\n",
    "#colunistas\n",
    "#a[2][7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "17b79264",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_col = pd.read_json('https://raw.githubusercontent.com/Faradezo/Reconhece_autor/main/textos/alexandre_garcia.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06c74dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Faradezo\\AppData\\Local\\Temp/ipykernel_7816/1367375677.py:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_col['texto_tratado'] = df_col['texto_tratado'].str.replace('[{}]'.format(string.punctuation), ' ')\n",
      "C:\\Users\\Faradezo\\AppData\\Local\\Temp/ipykernel_7816/1367375677.py:21: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df_col['texto_tratado'] = df_col['texto_tratado'].str.replace('[{}]'.format(string.digits), '')\n"
     ]
    }
   ],
   "source": [
    "#Faz a contagem de palavras por cada linha do artigo\n",
    "df_col['words'] = df_col['texto'].str.split().str.len()\n",
    "\n",
    "#Elimina qualquer artigo que tenha gerado menos de 100 palavras\n",
    "df_col = df_col.loc[(df_col['words'] > 100)]\n",
    "\n",
    "stop = stopwords.words('portuguese')\n",
    "# stop.append('nao')\n",
    "stop2 = list()\n",
    "for word in stop:\n",
    "  stop2.append(unidecode(word))\n",
    "  \n",
    "stemmer = nltk.stem.RSLPStemmer()\n",
    "# remoção de palavras de parada\n",
    "df_col['texto_tratado'] = df_col['texto'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "#Transforma em minuscula e remoção de acentos \n",
    "df_col['texto_tratado'] = df_col['texto_tratado'].str.lower().apply(lambda x: unidecode(x))\n",
    "# remocação de pontuaçao\n",
    "df_col['texto_tratado'] = df_col['texto_tratado'].str.replace('[{}]'.format(string.punctuation), ' ')\n",
    "# remocação de numeros\n",
    "df_col['texto_tratado'] = df_col['texto_tratado'].str.replace('[{}]'.format(string.digits), '')\n",
    "# remoção de palavras de parada (repescagem)\n",
    "df_col['texto_tratado'] = df_col['texto_tratado'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d909a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer()\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "#Arnaldo Jabor\n",
    "XCOL = vectorizer.fit_transform(df_col['texto_tratado'])\n",
    "vocabulary_col = vectorizer.get_feature_names()\n",
    "pdXCOL = pd.DataFrame(data=XCOL.toarray(), columns=vocabulary_col) #.iloc[:,0::2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "704f9993",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tudo</th>\n",
       "      <td>1141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ser</th>\n",
       "      <td>1112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mundo</th>\n",
       "      <td>1022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hoje</th>\n",
       "      <td>972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vida</th>\n",
       "      <td>806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pais</th>\n",
       "      <td>790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>anos</th>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nada</th>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grande</th>\n",
       "      <td>715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brasil</th>\n",
       "      <td>686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bem</th>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sempre</th>\n",
       "      <td>637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pois</th>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>porque</th>\n",
       "      <td>576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contra</th>\n",
       "      <td>557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sobre</th>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>onde</th>\n",
       "      <td>506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mal</th>\n",
       "      <td>483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dia</th>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>agora</th>\n",
       "      <td>470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nunca</th>\n",
       "      <td>453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vai</th>\n",
       "      <td>452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tempo</th>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>todos</th>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>politica</th>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ainda</th>\n",
       "      <td>420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outro</th>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pode</th>\n",
       "      <td>417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>assim</th>\n",
       "      <td>408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          total\n",
       "tudo       1141\n",
       "ser        1112\n",
       "mundo      1022\n",
       "hoje        972\n",
       "vida        806\n",
       "pais        790\n",
       "anos        735\n",
       "nada        725\n",
       "grande      715\n",
       "brasil      686\n",
       "bem         642\n",
       "sempre      637\n",
       "pois        577\n",
       "porque      576\n",
       "contra      557\n",
       "sobre       543\n",
       "onde        506\n",
       "mal         483\n",
       "dia         471\n",
       "agora       470\n",
       "nunca       453\n",
       "vai         452\n",
       "tempo       446\n",
       "ai          443\n",
       "todos       426\n",
       "politica    425\n",
       "ainda       420\n",
       "outro       418\n",
       "pode        417\n",
       "assim       408"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_XCOL = pdXCOL.T\n",
    "df_XCOL['total'] = df_XCOL.sum(axis=1) \n",
    "\n",
    "# Transpoe a matriz para que as palavras fiquem como linhas, e cada documento como uma linha\n",
    "df_XCOL = pdXCOL.T\n",
    "\n",
    "#Calcula a quantidade de vezes que cada palavra é utilizada\n",
    "df_XCOL['total'] = df_XCOL.sum(axis=1)\n",
    "\n",
    "#Remove as colunas individuais de cada documento para gerar a tabela\n",
    "df_XCOL.drop(df_XCOL.columns[0:len(df_col['texto'])], axis=1, inplace=True)\n",
    "\n",
    "#Ordena o resultado final\n",
    "df_XCOL = df_XCOL.sort_values(by='total', ascending=False)\n",
    "\n",
    "#Gera a tabela\n",
    "df_XCOL.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bf0f380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 30\n",
    "\n",
    "# Transpoe a matriz para que as palavras fiquem como linhas, e cada documento como uma linha\n",
    "pdXCOLT = pdXCOL.T\n",
    "\n",
    "#Mantem o percentual de frequencia de uso para comparação\n",
    "pdXCOLT['presente'] = (pdXCOLT.ne(0).sum(axis=1) -1) / len(df_col['texto'])\n",
    "\n",
    "#Ordena\n",
    "pdXCOLT = pdXCOLT.sort_values(by='presente', ascending=False)\n",
    "\n",
    "#Mantem so as 30 primeiras palavras\n",
    "pdXCOLT = pdXCOLT.head(n_words)\n",
    "\n",
    "#mantem 15 textos aleatorios para comparar com os demais exemplos\n",
    "#sample_drop =  random.sample(range(0, len(df_col['texto'])), len(df_col['texto']) -0)\n",
    "#pdXCOLT.drop(columns=sample_drop, axis=1, inplace=True)\n",
    "#pdXCOLT.drop(columns='presente', axis=1, inplace=True)\n",
    "\n",
    "# Apaga todas as clunas, pois so queremos manter o indice para avaliar os demais textos\n",
    "pdXCOLT.drop(pdXCOLT.columns[0:424], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90755bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
